# LLM Configuration
# Choose your LLM provider: 'openai' or 'gemini'
LLM_PROVIDER=gemini

# API Keys (only the one for your chosen provider is required)
OPENAI_API_KEY=your_openai_api_key_here
GEMINI_API_KEY=your_gemini_api_key_here

# Model Configuration (optional - defaults are set per provider)
# For OpenAI: gpt-4o-mini, gpt-4o, gpt-3.5-turbo, etc.
# For Gemini: gemini-2.5-flash, gemini-1.5-pro, etc.
LLM_MODEL=gemini-2.5-flash

# Embedding Model
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Directory Configuration
DATA_DIRECTORY=./data
PERSIST_DIRECTORY=./chroma_db

# Chunking Configuration
CHUNK_SIZE=500
CHUNK_OVERLAP=50

# Query Configuration
MAX_CONTEXT_TOKENS=3000
TOP_K_RESULTS=5

# Server Configuration
ENVIRONMENT=development
PORT=8000
LOG_LEVEL=info

# CORS Configuration (comma-separated list of allowed origins)
# For development: http://localhost:3000
# For production: https://yourdomain.com
ALLOWED_ORIGINS=http://localhost:3000

# Frontend Configuration
NEXT_PUBLIC_VISTA_API_URL=http://localhost:8000
NEXT_PUBLIC_GITHUB_URL=https://github.com/yourusername
NEXT_PUBLIC_LINKEDIN_URL=https://www.linkedin.com/in/yourprofile/
NEXT_PUBLIC_EMAIL=your.email@example.com